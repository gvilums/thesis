Naive baseline:
- 11 Tasklets to fill pipeline
- Use default read/write buffer sizes (256), except if inputs require larger size
- optionally:
    - Use one reduction var per tasklet if they all fit into WRAM, else one global one with mutex
    - Use optimal approach for number of reduction variables






Thesis text outline:
- Introduction to the problem
    - PIM architectures are becoming more relevant
    - Programming PIM has not really been explored yet
    - UPMEM is a real pim architecture with a defined programming model,
      but even that is quite complicated and cumbersome for defining relatively simple operations
        - Have to handle various architectural details
- Core question: Can we explore higher-level programming models on PIM architectures (or, more specifically, UPMEM)?
- Architecture Background:
    - What is PIM all about?
    - What PIM architectures are there?
    - Why UPMEM?
        - Exists and can be benchmarked
        - Defined low-level programming interface
    - Core characteristics of UPMEM architecture
        - refer to PrIM paper
- Programming Model:
    - What kinds of programming models for parallel computation are there?
        - Different approaches exist:
            - Specialized to particular workloads or architectures
            - As general as possible
    - Go into detail of MapReduce model (as it is implemented here)
        - Basic functionality (map/filter/reduce)
        - What can/can't we do?
            - e.g. can't do dynamic memory allocation
    - Why choose MapReduce?
        - Relatively straight-forward and easy to understand/program
        - Fits upmem architecture quite well (due to lack of inter-dpu communication)
- Implementation:
    - Describe code generator approach
        - General structure of generated code
        - Core design decisions
    - Explore optimizations / choosing best possible size parameters
    - What are limitations of the UPMEM toolchain?
        - Copies to multiple DPUs are all forced to use the same size.
          This prevents us from collecting all of the results using
          a single copy operation, and requires moving the data in memory afterwards
- Evaluation:
    - Compare to hand-written PrIM benchmarks for applicable programs
        - sum, histogram-small, histogram-large, select (any others?)
    - Compare optimizations against internal benchmarks
        - Optimizations mostly don't seem to make much of a difference.
        - Kind of expected, as UPMEM is a compute-bound platform in general
        We see that:
        - When dpu computation dominates, reading data makes a miniscule difference
    - Transfer overheads due to toolchain limtations
- Discussion
    - Other PIM architectures
    - Alternative programming models
    - Further work:
        - Better toolchain integration (heterogeneous compilation)
        - More intelligent compilation, cost models, etc.
- Conclusion
